{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing zarr-formatted Daymet data on Azure\n",
    "\n",
    "The Daymet dataset contains daily minimum temperature, maximum temperature, precipitation, shortwave radiation, vapor pressure, snow water equivalent, and day length at 1km resolution for North America. The dataset covers the period from January 1, 1980 to December 31, 2019.\n",
    "\n",
    "The Daymet dataset is maintained at [daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328](daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1328) and mirrored on Azure Open Datasets at [aka.ms/ai4edata-daymet](aka.ms/ai4edata-daymet). Azure also provides a cloud-optimized version of the data in [Zarr](https://zarr.readthedocs.io/en/stable/) format, which can be read into an [xarray](http://xarray.pydata.org/en/stable/) [Dataset](http://xarray.pydata.org/en/stable/data-structures.html#dataset). If you just need a subset of the data, we recommend using xarray and Zarr to avoid downloading the full dataset unnecessarily.\n",
    "\n",
    "The datasets are available in the `daymeteuwest` storage account, in the `daymet-zarr` container.  Files are named according to `daymet-zarr/{frequency}/{region}.zarr`, where frequency is one of `{daily, monthly, annual}` and region is one of `{hi, na, pr}` (for Hawaii, CONUS, and Puerto Rico, respectively). For example, `daymet-zarr/daily/hi.zarr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard or standard-ish imports\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Less standard, but still pip- or conda-installable\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "\n",
    "# Neither of these are accessed directly, but both need to be installed; they're used\n",
    "# via fsspec\n",
    "import adlfs\n",
    "import zarr\n",
    "\n",
    "account_name = 'daymeteuwest'\n",
    "container_name = 'daymet-zarr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into an xarray Dataset\n",
    "\n",
    "We can lazily load the data into an `xarray.Dataset` by creating a zarr store with [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) and then reading it in with xarray. This only reads the metadata, so it's safe to call on a dataset that's larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fsspec.get_mapper('az://' + container_name + '/daily/hi.zarr', account_name=account_name)\n",
    "# consolidated=True speeds of reading the metadata\n",
    "ds = xr.open_zarr(store, consolidated=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the data\n",
    "\n",
    "Using xarray, we can quickly select subsets of the data, perform an aggregation, and plot the result. For example, we'll plot the average of the maximum temperature for the year 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ds.sel(time=\"2009\")[\"tmax\"].mean(dim=\"time\").plot.imshow(ax=ax, cmap=\"inferno\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can visualize the timeseries of the minimum temperature over the past decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ds.sel(time=slice(\"2010\", \"2019\"))['tmin'].mean(dim=[\"x\", \"y\"]).plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Each of the datasets is chunked to allow for parallel and out-of-core or distributed processing with [Dask](https://dask.org/). The different frequencies (daily, monthly, annual) are chunked so that each year is in a single chunk. The different regions in the `x` and `y` coordinates so that no single chunk is larger than about 250 MB, which is primarily important for the `na` region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['prcp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our `prcp` array has a shape `(14600, 584, 284)` where each chunk is `(365, 584, 284)`. Examining the store for monthly North America, we see the chunk each of a size of `(12, 1250, 1250)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_store = fsspec.get_mapper(\"az://\" + container_name + \"/monthly/na.zarr\",\n",
    "                             account_name=account_name)\n",
    "na = xr.open_zarr(na_store, consolidated=True)\n",
    "na['prcp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See http://xarray.pydata.org/en/stable/dask.html for more on how xarray uses Dask for parallel computing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
